{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(convergence_gradient_descent)=\n",
    "\n",
    "# Convergence of gradient descent\n",
    "\n",
    "A general analysis on the convergence of gradient descent can prove to be very difficult. We will focus on the case where $\\nabla f$ shows one or both of the following proerties\n",
    "\n",
    "1. Lipschitz continuous\n",
    "2. Strong convexity\n",
    "\n",
    "## Lipschitz continuity\n",
    "\n",
    "A function $f : \\mathbb{R}^{n} -> \\mathbb{R}$ is Lipschitz continuosu with Lipschitz constant $L$ if for any $x,y \\epsilon \\mathbb{R}^{n}$\n",
    "\n",
    "$$\n",
    "    | f(y) - f(x) | <= L \\Vert y-x \\Vert_{2}\n",
    "$$\n",
    "\n",
    "## Strong convexity\n",
    "\n",
    "A differentiable function $f : \\mathbb{R}^{n} -> \\mathbb{R}$ is $\\mu$ strongly convex if for any $x,y \\epsilon \\mathbb{R}^{n}$\n",
    "\n",
    "$$\n",
    "    f(y) \\geq f(x) + \\nabla f(x)^{T} (y-x) + \\frac{\\mu}{2} \\Vert y-x \\Vert_{2}^{2}\n",
    "$$\n",
    "\n",
    "This is equivalent to \n",
    "\n",
    "$$\n",
    "    \\left< \\nabla f(x) - \\nabla f(y), x-y \\right> \\geq \\alpha \\Vert y-x \\Vert_{2}^{2}\n",
    "$$\n",
    "\n",
    "\n",
    "*Note*\n",
    "\n",
    "Compare the first condition above to the first order condition for convex functions\n",
    "\n",
    "$$\n",
    "    f(y) \\geq f(x) + \\nabla f(x)^{T}(y-x)\n",
    "$$\n",
    "\n",
    "The additional term gives a stronger lower bound ( probably which lead to the name strong convexity )\n",
    "\n",
    "## Convergence under Lipschitz continuity\n",
    "\n",
    "In this section we prove that gradient descent converges in $O(1/\\epsilon)$ steps to an $\\epsilon$ approximate solution when $\\nabla f$ is Lipschitz continuous\n",
    "\n",
    "We need a few results to help us with the proof\n",
    "\n",
    "### Monotonicity of gradient\n",
    "\n",
    "(monotonicity_of_gradient)=\n",
    "**Lemma 1.1** \n",
    "\n",
    "A differentiable function $f : \\mathbb{R}^{n} -> \\mathbb{R}$ is convex iff\n",
    "\n",
    "$$\n",
    "    (\\nabla f(y) - \\nabla f(x))^{T}(y-x) >= 0\n",
    "$$\n",
    "\n",
    "**Proof**\n",
    "\n",
    "Part 1: Convexity => $(\\nabla f(y) - \\nabla f(x))^{T}(y-x) >= 0$\n",
    "\n",
    "If f is convex by first order condition\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "f(x) \\geq f(y) + \\nabla f(y)^{T}(x-y) \\\\\n",
    "f(y) \\geq f(x) + \\nabla f(x)^{T}(y-x)\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Adding the two equations above gives us\n",
    "\n",
    "$$\n",
    "    \\left( \\nabla f(y)^{T} - \\nabla f(x)^{T} \\right) \\left( y-x \\right) \\geq 0\n",
    "$$\n",
    "\n",
    "Part 2: $(\\nabla f(y) - \\nabla f(x))^{T}(y-x) >= 0$ => Convexity\n",
    "\n",
    "Consider the restriction of the function f -> $g_{a,b}$\n",
    "\n",
    "$$\n",
    "    g_{a,b}(\\alpha): [0,1] -> \\mathbb{R}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    g_{a,b}(\\alpha) := f(\\alpha a + (1-\\alpha)b)\n",
    "$$\n",
    "\n",
    "$$\n",
    "    g'_{a,b}(\\alpha) = \\nabla f(\\alpha a + (1-\\alpha)b)^{T}(a-b)\n",
    "$$\n",
    "\n",
    "For any $\\alpha \\epsilon [0,1]$ we have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g'_{a,b}(\\alpha) - g'_{a,b}(0) &= \\left( \\nabla f(\\alpha a + (1-\\alpha)b) \\right) (a-b) \\\\\n",
    "    &= \\frac{1}{\\alpha} \\left( \\nabla f(\\alpha a + (1-\\alpha)b) \\right) \\left( \\alpha a - (1-\\alpha)b - b \\right) \\\\\n",
    "    &\\geq 0 (  since  \\nabla f(y) - \\nabla f(x))^{T}(y-x) >= 0 ) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Writing $f(x)$ in terms of the restriction and using the above inequality gives\n",
    "\n",
    "$$\n",
    "\\begin{align}    \n",
    "    f(x) &= g_{xy}(1) \\\\\n",
    "    &= g_{xy}(0) + \\int_{0}^{1} g'_{xy}(\\alpha)d\\alpha \\\\\n",
    "    &\\geq g_{xy}(0) + \\int_{0}^{1} g'_{xy}(0)d\\alpha \\\\\n",
    "    &\\geq g_{xy}(0) + g'_{xy}(0) \\\\\n",
    "    &=f(y) + \\nabla f(y)^{T}(x-y)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This is the first order condition for convexity => f is convex\n",
    "\n",
    "### First order upper bound\n",
    "(first_order_upper_bound)=\n",
    "**Theorem 1.1** \n",
    "\n",
    "If the gradient of a function is L-Lipschitz continuous with Lipschitz constant L\n",
    "\n",
    "$$\n",
    "    \\Vert \\nabla f(y) - \\nabla f(x) \\Vert_{2} <= L \\Vert y-x \\Vert_{2}\n",
    "$$\n",
    "\n",
    "then for any $x \\epsilon \\mathbb{R}^{n}$ the following holds\n",
    "\n",
    "$$\n",
    "    f(y) <= f(x) + \\nabla f(x)^{T}(y-x) + \\frac{L}{2}  \\Vert y-x \\Vert_{2}^{2}\n",
    "$$\n",
    "\n",
    "**Proof**\n",
    "\n",
    "Consider the function\n",
    "\n",
    "$$\n",
    "    g(x) := \\frac{L}{2} x^{T}x - f(x)\n",
    "$$\n",
    "\n",
    "For g\n",
    "\n",
    "$$\n",
    "    \\nabla g(x) = Lx - \\nabla f(x)\n",
    "$$\n",
    "\n",
    "For $x,y \\epsilon \\mathbb{R}^{n}$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\nabla g(y) - \\nabla g(x) &= Ly - \\nabla f(x) - LX + \\nabla f(x) \\\\\n",
    "    &= L(y-x) - \\left( \\nabla f(y) - \\nabla f(x) \\right) \\\\\n",
    "    \\left( \\nabla g(y) - \\nabla g(x) \\right )^{T}(y-x) &= L \\Vert y-x \\Vert_{2}^{2} - \\left( \\nabla f(y) - \\nabla f(x) \\right)^{T}(y-x) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "By Cauchy Schwarz inequality\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\left< \\nabla f(y) - \\nabla f(x), y-x \\right> & \\leq \\Vert \\nabla f(y) - \\nabla f(x) \\Vert \\Vert y-x \\Vert \\\\\n",
    "    & \\leq L \\Vert y-x \\Vert^{2}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This implies\n",
    "\n",
    "$$\n",
    "    \\left( \\nabla g(y) - \\nabla g(x) \\right)^{T} (y-x) \\geq 0\n",
    "$$\n",
    "\n",
    "The above implies g is convex by the monotonicity of the gradient lemma proved above\n",
    "\n",
    "By first order condition for convexity\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\frac{L}{2} y^{T}y - f(y) &= g(y) \\\\\n",
    "    &\\geq g(x) + \\nabla g(x)^{T}(y-x) \\\\\n",
    "    &= \\frac{L}{2} x^{T}x - f(x) +(Lx - \\nabla f(x))^{T}(y-x)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This implies\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    f(y) &\\leq f(x) + \\nabla f(x)^{T}(y-x) + \\frac{L}{2}(y^{T}y - x^{T}x) - Lx^{T}(y-x) \\\\\n",
    "    &\\leq  f(x) + \\nabla f(x)^{T}(y-x) + \\frac{L}{2}(y^{T}y - x^{T}x) - 2x^{T}y + 2x^{T}x) \\\\\n",
    "    &= f(x) + \\nabla f(x)^{T}(y-x) + \\frac{L}{2}(y^{T}y + x^{T}x) - 2x^{T}y) \\\\\n",
    "    &= f(x) + \\nabla f(x)^{T}(y-x) + \\frac{L}{2}(y-x)^{T}(y-x) \\\\\n",
    "    &= f(x) + \\nabla f(x)^{T}(y-x) + \\frac{L}{2} \\Vert y-x \\Vert_{2}^{2}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Hence proved.\n",
    "\n",
    "### Convergence of successive iterates\n",
    "(convergence_of_successive_iterates)=\n",
    "**Theorem 1.2** \n",
    "\n",
    "Let $x_{i}$ be the ith iteration of gradient descent and $\\alpha_{i} >= 0$. if $\\nabla f$ is L-Lipschitz continuous then\n",
    "\n",
    "$$\n",
    "    f(x_{k+1}) <= f(x_{k}) - \\alpha_{k} ( 1 - \\frac{\\alpha_{k}L}{2} ) \\Vert \\nabla f(x_{k}) \\Vert_2^{2}\n",
    "$$\n",
    "\n",
    "**Proof**\n",
    "\n",
    "Applying the first order upper bound from theorem 1.1 we have\n",
    "\n",
    "$$\n",
    "    f(x_{k+1}) <= f(x_{k}) + \\nabla f(x_{k})^{T}(x_{k+1} - x_{k}) + \\frac{L}{2} \\Vert x_{k+1} - x_{k} \\Vert_{2}^{2}\n",
    "$$\n",
    "\n",
    "In gradient descent the iterates are computed as\n",
    "\n",
    "$$\n",
    "    x_{k+1} = x_{k} - \\alpha_{k} \\nabla f(x_{k})\n",
    "$$\n",
    "\n",
    "This implies\n",
    "\n",
    "$$\n",
    "  x_{k+1} - x_{k} = - \\alpha_{k} \\nabla f(x_{k})\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    f(x_{k+1}) &\\leq f(x_{k}) + \\nabla f(x_{k})^{T}(-\\alpha_{k}\\nabla f(x_{k})) + \\frac{L}{2}\\alpha_{k}^{2} \\Vert \\nabla f(x_{k}) \\Vert_{2}^{2} \\\\\n",
    "    &\\leq f(x_{k}) - \\alpha_{k} \\Vert \\nabla f(x_{k}) \\Vert_{2}^{2} + \\frac{L}{2}\\alpha_{k}^{2} \\Vert \\nabla f(x_{k}) \\Vert_{2}^{2} \\\\\n",
    "    &\\leq f(x_{k}) - \\alpha_{k} \\left( 1- \\frac{\\alpha_{k} L}{2} \\right) \\Vert \\nabla f(x_{k}) \\Vert_{2}^{2} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Hence proved\n",
    "\n",
    "### Gradient descent is a descent method\n",
    "(gradient_descent_is_a_descent_method)=\n",
    "\n",
    "**Corollary 1.1** Gradient descent is a descent method if $0 \\lt \\alpha_{k} \\lt \\frac{2}{L}$\n",
    "\n",
    "**Proof**\n",
    "\n",
    "Substituting $0 \\lt \\alpha_{k} \\lt \\frac{2}{L}$ into the inequality from the above theorem gives us the result\n",
    "\n",
    "### Convergence rate under Lipschitz continuity\n",
    "(convergence_rate_under_lipschitz)=\n",
    "\n",
    "**Theorem 1.3 ( Main result )** : Assume \n",
    "\n",
    "- f is convex\n",
    "- $\\nabla f$ is L-Lipschitzz continuous \n",
    "- There exists a point $x^{*}$ at which f achieves a finite minimum\n",
    "\n",
    "If we setup the step size of gradient descent $\\alpha_{k} = \\alpha = \\frac{1}{L}$ for every iteration then\n",
    "\n",
    "$$\n",
    "    f(x_{k}) - f(x_{*}) <= \\frac{ \\Vert x_{0}-x_{*} \\Vert^{2}}{2\\alpha k}\n",
    "$$\n",
    "\n",
    "**Proof**\n",
    "\n",
    "By first order characterisation of convexity\n",
    "\n",
    "$$\n",
    "    f(x_{k-1}) + \\nabla f(x_{k-1})^{T}(x_{*} - x_{k-1}) \\leq f(x_{*})\n",
    "$$\n",
    "\n",
    "Using the previous corollary\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    f(x_{k}) & \\leq f(x_{k-1}) - \\frac{\\alpha}{2} \\Vert \\nabla f(x_{k-1}) \\Vert_{2}^{2} \\\\\n",
    "    f(x_{k-1}) & \\geq f(x_{k}) + \\frac{\\alpha}{2} \\Vert \\nabla f(x_{k-1}) \\Vert_{2}^{2}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Combining the two\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(x_{k}) + \\frac{\\alpha}{2} \\Vert \\nabla f(x_{k-1}) \\Vert_{2}^{2} + \\nabla f(x_{k-1})^{T}(x_{*} - x_{k-1}) \\leq f(x_{*}) \\\\\n",
    "f(x_{k}) - f(x_{*}) \\leq \\nabla f(x_{k-1})^{T}(x_{*} - x_{k-1}) - \\frac{\\alpha}{2} \\Vert \\nabla f(x_{k-1}) \\Vert_{2}^{2} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Lets expand the RHS term\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    RHS &= \\nabla f(x_{k-1})^{T}(x_{*} - x_{k-1}) - \\frac{\\alpha}{2} \\Vert \\nabla f(x_{k-1}) \\Vert_{2}^{2} \\\\\n",
    "    &= \\frac{1}{2\\alpha} \\left( 2\\alpha \\nabla f(x_{k-1})^{T}(x_{k-1} - x_{*}) - \\alpha^{2} \\Vert \\nabla f(x_{k-1}) \\Vert_{2}^{2}  \\right) \\\\\n",
    "    &= \\frac{1}{2\\alpha} \\left( \\Vert x_{k-1} - x_{*} \\Vert_{2}^{2} - \\Vert x_{k-1} - x_{*} \\Vert_{2}^{2} + 2\\alpha \\nabla f(x_{k-1})^{T}(x_{k-1} - x_{*}) -\\alpha^{2} \\Vert \\nabla f(x_{k-1})\\Vert_{2}^{2} \\right) \\\\\n",
    "    &= \\frac{1}{2\\alpha} \\left( \\Vert x_{k-1} - x_{*} \\Vert_{2}^{2} - \\Vert x_{k-1} - x_{*} - \\alpha \\nabla f(x_{k-1}) \\Vert_{2}^{2}  \\right) \\\\\n",
    "    &= \\frac{1}{2\\alpha} \\left( \\Vert x_{k-1}-x_{*} \\Vert_{2}^{2} - \\Vert x_{k}-x_{*} \\Vert_{2}^{2} \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Hence we have\n",
    "\n",
    "$$\n",
    "    f(x_{k}) - f(x_{*}) \\leq \\frac{1}{2\\alpha} \\left( \\Vert x_{k-1}-x_{*} \\Vert_{2}^{2} - \\Vert x_{k}-x_{*} \\Vert_{2}^{2} \\right)\n",
    "$$\n",
    "\n",
    "By previous corollary we know that $\\left( f(x_{k}) \\right)_{k=1}^{\\inf}$ is decreasing, hence\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    f(x_{k}) & \\leq \\sum_{i=1}^{k} \\frac{1}{k} f(x_{i}) \\\\\n",
    "    f(x_{k}) - f(x_{*}) & \\leq \\left[  \\sum_{i=1}^{k} \\frac{1}{k} f(x_{i}) \\right]  - f(x_{*}) \\\\\n",
    "    &= \\frac{1}{k} \\left( \\sum_{i=1}^{k} f(x_{i}) -f(x_{*}) \\right) \\\\\n",
    "    &= \\frac{1}{2 \\alpha k} \\left( \\Vert x_{0}-x_{*} \\Vert_{2}^{2} - \\Vert x_{k}-x_{*} \\Vert_{2}^{2} \\right) \\\\\n",
    "    & \\leq \\frac{\\Vert x_{0} - x_{*} \\Vert_{2}^{2}}{2 \\alpha k}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Hence we have\n",
    "\n",
    "$$\n",
    "    f(x_{k}) - f(x_{*}) \\leq \\frac{\\Vert x_{0} - x_{*} \\Vert_{2}^{2}}{2 \\alpha k}\n",
    "$$\n",
    "\n",
    "Theorem 1.3 shows that if we know the Lipschitz constant of the gradient then we can choose a fixed step $\\alpha$ to get arbitrarily close to the minimum. In particular to get an $\\epsilon$ approximate solution we would need $O(\\frac{1}{\\epsilon})$ steps\n",
    "\n",
    "---\n",
    "\n",
    "**Note: Why we need constant $\\alpha_{k} = \\frac{1}{L}$**\n",
    "\n",
    "We previously showed that gradient descent is a descent method when $0 \\lt \\alpha_{k} \\lt \\frac{2}{L}$. However we are using $\\alpha_{k} = \\frac{1}{L}$ here. One may wonder why that is the case.\n",
    "\n",
    "Say $\\alpha_{k} = \\frac{2}{L} - \\epsilon_{k}$ and $\\epsilon_{min} = min_{k} \\epsilon_{k} \\gt 0$. Then repeating parts of the above proof we get\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    f(x_{k}) & \\leq f(x_{k-1}) - \\alpha_{k} \\left( 1 - \\frac{\\alpha_{k}L}{2} \\right) \\Vert \\nabla f(x_{k-1}) \\Vert^{2} \\\\\n",
    "    f(x_{k}) - f(x_{k-1}) & \\leq \\frac{-\\alpha_{k}L\\epsilon_{min}}{2} \\Vert \\nabla f(x_{k-1}) \\Vert^{2} \\\\\n",
    "    f(x_{k-1}) & \\geq f(x_{k}) + \\frac{\\alpha_{k}L\\epsilon_{min}}{2} \\Vert \\nabla f(x_{k-1}) \\Vert^{2} \\\\ \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "By first order condition for convexity\n",
    "\n",
    "$$\n",
    "    f(x_{k-1}) \\leq f(x_{*}) - \\nabla f(x_{k-1})^{T}(x_{*} - x_{k-1})\n",
    "$$\n",
    "\n",
    "Combining the two we have\n",
    "\n",
    "$$\n",
    "   f(x_{k}) + \\frac{\\alpha_{k}L\\epsilon_{min}}{2} \\Vert \\nabla f(x_{k-1}) \\Vert^{2} \\leq f(x_{k-1}) \\leq f(x_{*}) - \\nabla f(x_{k-1})^{T}(x_{*} - x_{k-1})\n",
    "$$\n",
    "\n",
    "Hence\n",
    "\n",
    "$$\n",
    "   f(x_{k}) + \\frac{\\alpha_{k}L\\epsilon_{min}}{2} \\Vert \\nabla f(x_{k-1}) \\Vert^{2} \\leq f(x_{*}) - \\nabla f(x_{k-1})^{T}(x_{*} - x_{k-1}) \n",
    "$$\n",
    "\n",
    "This leads to \n",
    "\n",
    "$$\n",
    "    f(x_{k}) - f(x_{*}) \\leq \\frac{1}{2\\alpha_{k-1}L\\epsilon_{min}} \\left( \\Vert x_{k-1}-x_{*} \\Vert^{2} - \\Vert x_{k-1} - \\alpha_{k}L\\epsilon_{min} \\nabla f(x_{k-1}) - x_{*} \\Vert^{2} \\right)\n",
    "$$\n",
    "\n",
    "When $\\alpha_{k} = \\frac{1}{L}$ then $\\epsilon_{k} = \\epsilon_{min} = \\frac{1}{L}$, the expression on the right becomes\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "   RHS &= \\Vert x_{k-1} - \\alpha_{k}L\\epsilon_{min} \\nabla f(x_{k-1}) - x_{*} \\Vert^{2} \\\\\n",
    "   &= \\Vert x_{k-1} - \\alpha_{k} \\nabla f(x_{k-1}) - x_{*} \\Vert^{2} \\\\\n",
    "   & = \\Vert x_{k} - x_{*} \\Vert^{2}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "To leverage that $x_{k} = x_{k-1} - \\alpha_{k} \\nabla f(x_{k-1})$ we need $L\\epsilon_{min} = 1$ or $\\epsilon_{min} = \\frac{1}{L}$. This would imply $\\alpha_{k} \\leq \\frac{1}{L}$. We want the highest learning rate possible with guarantees of convergence. Hence we choose $\\alpha_{k} = \\frac{1}{L}$ constant.\n",
    "\n",
    "---\n",
    "\n",
    "In practice we may not know the Lipschitz constant of a function analytically. In which case there are 2 ways to proceed forward:\n",
    "\n",
    "1. Find an upper bound for the Lipschitz constant if possible\n",
    "2. Use backtracking line search ( Armijo rule  )\n",
    "    We can show that backtracking line search is capable of hitting the $\\alpha_{k} <= \\frac{1}{L}$ condition\n",
    "\n",
    "For the latter we have the following results\n",
    "\n",
    "**Lemma 1.2** Lower bound on step size with backtracking search\n",
    "\n",
    "If the gradient of a function $f: \\mathbb{R}^{n} -> \\mathbb{R}$ is Lipschitz continuous with Lipschitz constant L, the step size obtained by backtracking line search with $\\eta = 0.5$ satisfies\n",
    "\n",
    "$$\n",
    "    \\alpha_{k} >= \\alpha_{min} := min \\left( \\alpha^{0}, \\frac{\\beta}{L} \\right) \n",
    "$$\n",
    "\n",
    "**Theorem 1.4** Convergence with backtracking line search\n",
    "\n",
    "If f is convex and $\\nabla f$ is L-Lipschitz continuous, gradient descent with backtracking line search produces a sequence of points that satisfy\n",
    "\n",
    "$$\n",
    "    f(x_{k}) - f(x_{*}) <= \\frac{\\Vert x_{0}-x_{*} \\Vert_{2}^{2}}{2\\alpha_{min}k}\n",
    "$$\n",
    "\n",
    "We skip the proofs of the above theorem and lemma. They are available in [Section 3.2 of CF.Granda's lecture notes](https://cims.nyu.edu/~cfgranda/pages/OBDA_fall17/notes/convex_optimization.pdf)\n",
    "\n",
    "## Convergence under Lipschitz continuity and strong convexity\n",
    "\n",
    "**Theorem 1.5**\n",
    "\n",
    "Let f be L-smooth and $\\mu$ strongly convex. From a given $x_{0} \\epsilon \\mathbb{R}^{n}$ and $0 \\lt \\alpha \\leq \\frac{1}{L}$, the iterates\n",
    "\n",
    "$$\n",
    "    x_{t+1} = x_{t} - \\alpha \\nabla f(x_{t})\n",
    "$$\n",
    "\n",
    "converge according to \n",
    "\n",
    "$$\n",
    "    \\Vert x_{t+1} - x_{*} \\Vert_{2}^{2} \\leq (1-\\alpha \\mu)^{t+1} \\Vert x^{0} - x^{*} \\Vert_{2}^{2}    \n",
    "$$\n",
    "\n",
    "In particular when $\\alpha = \\frac{1}{L}$, the iterates converge linearly with a rate of $\\frac{\\mu}{L}$\n",
    "\n",
    "We skip the proof here. It can be found in [Robert Gower's notes](https://gowerrobert.github.io/pdf/M2_statistique_optimisation/grad_conv.pdf)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "source_map": [
   10
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 4
}