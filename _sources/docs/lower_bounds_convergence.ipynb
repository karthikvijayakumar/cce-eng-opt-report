{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(lower_bounds_convergence)=\n",
    "\n",
    "# Lower bounds on convergence\n",
    "\n",
    "Before we state the theorem on lower bounds, lets first define broadly what first order methods are\n",
    "\n",
    "In gradient descent\n",
    "\n",
    "$$\n",
    "    x_{k+1} = x_{k} - \\alpha_{k} \\nabla f(x_{k}); \\alpha \\epsilon \\mathbb{R}^{+}\n",
    "$$\n",
    "\n",
    "This means\n",
    "\n",
    "$$\n",
    "    x_{k+1} = x_{0} - \\sum_{i=1}^{k} \\alpha_{i} \\nabla f(x_{i})\n",
    "$$\n",
    "\n",
    "The k'th iterate is a linear combination of the first iterate and gradients of previous iterates\n",
    "\n",
    "\n",
    "In standard gradient descent each component of the gradient is multiplied by the same step size. This need not be the case though. We could multiply each component of the gradient with different scaling factors. This leads to the general definiion of first order methods\n",
    "\n",
    "\n",
    "**First order method**\n",
    "\n",
    "Iterate using \n",
    "\n",
    "$$\n",
    "    x_{k+1} = x_{k} - \\Gamma_{k} \\nabla f(x_{k}); \\Gamma_{ii} \\epsilon \\mathbb{R}^{+}\n",
    "$$\n",
    "\n",
    "which becomes \n",
    "\n",
    "$$\n",
    "    x_{k+1} = x_{0} - \\sum_{i=1}^{k} \\Gamma_{i} \\nabla f(x_{i})\n",
    "$$\n",
    "\n",
    "where $\\Gamma$ is a diagonal matrix\n",
    "\n",
    "**Theorem 1.5 ( Nesterov's lower bound )**\n",
    "\n",
    "There exist convex functions with L-Lipschitz continuous gradients such that for any first order method we have\n",
    "\n",
    "$$\n",
    "    f(x_{k}) - f(x_{*}) >= 3L\\frac{\\Vert x_{0} - x_{*} \\Vert_{2}^{2}}{32(k+1)^2}\n",
    "$$\n",
    "\n",
    "The above theorem implies using first order methods we can need $O\\left( \\frac{1}{\\sqrt \\epsilon} \\right)$ iterations to reach an $\\epsilon$ approximate solution\n",
    "\n",
    "Infact Nesterov's showed that the convergence $O\\left( \\frac{1}{\\sqrt \\epsilon} \\right)$ is optimal and gave the algorithm for it, which is accelerated gradient descent\n",
    "\n",
    "The proof of the Nesterov's theorem is quite involved and we will skip it here. Details can be found in [section 2.1.2 of Nesterov's text](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.693.855&rep=rep1&type=pdf)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "source_map": [
   10
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 4
}